# Language-Driven Graphs for Short Video Similarity

![Project Banner](figures/result_graph.png)

*Final comparison of the semantic cohesion of similarity graphs built from human descriptions, AI-generated (LLM) descriptions, and visual features (CLIP).*

## Overview

# Authors
> *Juliano Koji Yugoshi¹², Ricardo Marcondes Marcacini¹*  
> ¹Institute of Mathematical and Computer Sciences (ICMC), University of São Paulo (USP)  
> ²Campus de Três Lagoas (CPTL), Federal University of Mato Grosso do Sul (UFMS)  

This project investigates the effectiveness of different modalities in building similarity graphs for short videos. Using the **MSR-VTT** dataset, we construct and evaluate three types of graphs:

1.  **Visual Graph:** Based on cosine similarity between visual embeddings from CLIP.
2.  **Human Description Graph:** Based on semantic similarity (SBERT) of human-provided descriptions.
3.  **AI-Generated Description Graph:** Based on semantic similarity (SBERT) of descriptions generated by an LLM (`smol-vlm-2.0`).

### Visualizing the Structural Trade-off

The graph structures reveal a clear trade-off between visual purity and semantic richness. This is evident in the 1-hop neighborhoods of anchor videos, shown below for the "Music" and "People" categories across the three modalities.

*   **Visual (CLIP) Graphs (Left Column):** These graphs are extremely dense with high local purity, connecting almost exclusively to same-category videos. This confirms that visual features excel at grouping visually similar content but create a convoluted, "hairball-like" structure that lacks nuanced semantic connections.

*   **Text-Based Graphs (Center and Right Columns):** In sharp contrast, the text-based graphs are sparser, replacing dense local connections with meaningful, long-range semantic links. They connect videos across categories based on thematic or narrative similarity. For example, the `LLM-Text` graph connects the "Music" video to a conceptually related dance video. More impressively, for the "People" anchor, it creates a narrative link to a video of a person crying, based on the anchor's descriptive text "YOU CAN'T HIDE YOUR FEELINGS".

These visualizations confirm that LLMs produce a graph structure that trades categorical purity for a semantically richer topology, one that aligns with human thematic and narrative reasoning.

![Music Video Neighborhood](figures/neighborhood_analysis_video1118.png)
*Figure 1: Neighborhood comparison for a "Music" anchor video.*

![People Video Neighborhood](figures/neighborhood_analysis_video2753.png)
*Figure 2: Neighborhood comparison for a "People" anchor video.*

## Workflow and Execution

The process is divided into two Jupyter Notebooks, designed to be run sequentially in **Google Colab**.

### Notebook 1: `notebooks/01_Data_Preprocessing_and_Feature_Extraction.ipynb` (Optional)

&nbsp; &nbsp; This notebook performs the complete preprocessing pipeline, starting from the raw MSR-VTT data. **You only need to run this if you want to replicate the feature extraction from scratch.**

### Notebook 2: `notebooks/02_Graph_Analysis_and_Evaluation.ipynb` (Main)

&nbsp; &nbsp; This notebook conducts the core analysis. It loads the pre-processed data and features, builds the graphs, evaluates their semantic cohesion, and generates the final results plot.

## How to Reproduce the Results

### Step 1: Google Drive Environment Setup

1.  **Clone the Repository to Your Local Machine:**
    ```bash  
    git clone https://github.com/juliano-yugoshi/languagedrivengraphs.git

2.  **Create the Project Folder in Your Google Drive:**
    In the root of your Google Drive (`My Drive`), create the following folder structure: My Drive/D/Dataset/

3.  **Upload the Project Files:**
Upload all `.ipynb`, `.csv`, `.xlsx` files and the `figures/` folder that you cloned from GitHub into the `My Drive/D/Dataset/` directory you just created.

4.  **Add Shortcuts to Data and Features:**
Open the shared Google Drive links below. For each link, instead of downloading, click **"Add shortcut to Drive"** (or the triangle icon with a `+`) and select the `My Drive/D/Dataset/` folder as the destination.

**Required Shortcuts for Analysis (Notebook 2 - Recommended):**
*   `Keyframes/` ([Link](https://drive.google.com/drive/folders/1jiHTEsbit8o5WyVbcNYdRz3I995B8rBs?usp=sharing))
*   `MSRVTT_Features_ViT_L_14_Aggregated/` ([Link](https://drive.google.com/drive/folders/1USB4NbvxpCL_V-RFfqrClmMlPrWggvfu?usp=sharing))
*   `MSRVTT_Features_ViT_L_14_Sequential/` ([Link](https://drive.google.com/drive/folders/1TpshF89NqtFFBqb-IadriWaLu8Eyi5cL?usp=sharing))

**Shortcuts for Full Feature Generation (Optional):**
If you wish to run Notebook 1 from scratch, also add shortcuts for the raw data:
*   `TrainValVideo.zip` ([Link](https://drive.google.com/file/d/1rt4YDdhRblFvYpr3xdvHtff4-jqEWO_6/view?usp=sharing))
*   `train_val_videodatainfo.json`

### Step 2: Running the Notebooks

**Option A: Run Analysis Only (Recommended)**

1.  After setting up your Drive environment with the shortcuts, open the `notebooks/02_Graph_Analysis_and_Evaluation.ipynb` notebook in Google Colab.
2.  **Important Action:** In the Google Colab file browser (left-side panel), click the "Upload to session storage" icon and select the `MSRVTT_dados_compilados_com_features.xlsx` file from your local machine. This will make it directly accessible at the `/content/` path.
3.  Run all cells in the notebook to perform the complete analysis and generate the final results.

**Option B: Full Generation from Scratch**

1.  Ensure the shortcuts for the raw data (`TrainValVideo.zip` and the `.json` file) have been added to your Drive.
2.  Open the `notebooks/01_Data_Preprocessing_and_Feature_Extraction.ipynb` notebook in Google Colab with a GPU runtime (T4 or higher).
3.  Run all cells. This process is time-consuming and may take several hours. The outputs, including the `MSRVTT_dados_compilados_com_features.xlsx` file, will be saved to your Google Drive.
4.  Once complete, proceed with **Option A**, uploading the newly generated `.xlsx` file to the Colab session as instructed.

## Dependencies

The main libraries are installed directly within the notebooks using `!pip install` commands. Key dependencies include: `transformers`, `torch`, `sentence-transformers`, `networkx`, `pandas`, `scikit-learn`, `matplotlib`, `seaborn`, and `tqdm`.


